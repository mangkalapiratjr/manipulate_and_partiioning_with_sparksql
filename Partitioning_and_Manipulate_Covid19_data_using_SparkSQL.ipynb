{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Partitioning and Manipulate Covid19 data_using SparkSQL.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CbNSf2EMj9ZJ"
      },
      "source": [
        "# **Partitioning Covid19 cases data using SparkSQL + AWS S3**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1hgcFsAkMMH"
      },
      "source": [
        "**Steps**\n",
        "1. Import Covid19 dataset from AWS S3 (using datasource from ourworldindata.org)\n",
        "2. Check missing values\n",
        "3. Fill missing values with 0 and calculate cumulative total cases and total deaths as new columns\n",
        "4. Partitioning DataFrame by year of cases\n",
        "5. Save output as csv file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rpqXDy9CdixR"
      },
      "source": [
        "# Install libraries to use\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c07J97UYg1ZW",
        "outputId": "7c832631-3dcd-4b32-ccf2-ad0bcf5fc4b2"
      },
      "source": [
        "pip install pyspark"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.1.2.tar.gz (212.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 212.4 MB 64 kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9\n",
            "  Downloading py4j-0.10.9-py2.py3-none-any.whl (198 kB)\n",
            "\u001b[K     |████████████████████████████████| 198 kB 50.8 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.1.2-py2.py3-none-any.whl size=212880768 sha256=c3304539af53bbff6c7ea9d0d278761a5441b9885aab83e34d316bd837236b8d\n",
            "  Stored in directory: /root/.cache/pip/wheels/a5/0a/c1/9561f6fecb759579a7d863dcd846daaa95f598744e71b02c77\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9 pyspark-3.1.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CDA_nXx_cxa3",
        "outputId": "c1b3e081-0f6d-4ccc-bd98-5c3930be91d0"
      },
      "source": [
        "pip install boto3"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting boto3\n",
            "  Downloading boto3-1.18.22-py3-none-any.whl (131 kB)\n",
            "\u001b[K     |████████████████████████████████| 131 kB 4.9 MB/s \n",
            "\u001b[?25hCollecting botocore<1.22.0,>=1.21.22\n",
            "  Downloading botocore-1.21.22-py3-none-any.whl (7.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.8 MB 46.0 MB/s \n",
            "\u001b[?25hCollecting s3transfer<0.6.0,>=0.5.0\n",
            "  Downloading s3transfer-0.5.0-py3-none-any.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 8.0 MB/s \n",
            "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n",
            "Collecting urllib3<1.27,>=1.25.4\n",
            "  Downloading urllib3-1.26.6-py2.py3-none-any.whl (138 kB)\n",
            "\u001b[K     |████████████████████████████████| 138 kB 74.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.22.0,>=1.21.22->boto3) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.22.0,>=1.21.22->boto3) (1.15.0)\n",
            "Installing collected packages: urllib3, jmespath, botocore, s3transfer, boto3\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "requests 2.23.0 requires urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1, but you have urllib3 1.26.6 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed boto3-1.18.22 botocore-1.21.22 jmespath-0.10.0 s3transfer-0.5.0 urllib3-1.26.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OgU4VB3TduLG"
      },
      "source": [
        "# Import Data from AWS S3 and Create Spark Session"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nb_AzR-hd9Y-"
      },
      "source": [
        "import boto3\n",
        "\n",
        "s3 = boto3.client('s3', aws_access_key_id = AWS_ACCESS_KEY, aws_secret_access_key = AWS_SECRET_KEY)\n",
        "s3.download_file(Bucket='mj-dataset', Filename='owid-covid-data.csv', Key='owid-covid-data.csv')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F02H35TYhRDs"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, count, when, isnan, isnull\n",
        "\n",
        "spark = SparkSession.builder.getOrCreate()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQUnLdT3h6Wt"
      },
      "source": [
        "data_raw = spark.read.csv('owid-covid-data.csv', header=True, inferSchema=True)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OuexCG9SoQc1"
      },
      "source": [
        "# Handling missing values and calculate cumulative cases"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ro1ZcDPzcgzj",
        "outputId": "e9e7a456-255e-4f66-9ce4-76670517b21d"
      },
      "source": [
        "# Create TempView\n",
        "data_raw.createOrReplaceTempView('covid19')\n",
        "\n",
        "# Check NULL value\n",
        "spark.sql(\"\"\"\n",
        "select SUM(IF(ISNULL(date), 1, 0)) as null_date, SUM(IF(ISNULL(location), 1, 0)) as null_country , SUM(IF(ISNULL(new_cases), 1,0)) as null_new_cases, SUM(IF(ISNULL(new_deaths), 1, 0)) as null_new_deaths from covid19\n",
        "\"\"\").show()\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------+------------+--------------+---------------+\n",
            "|null_date|null_country|null_new_cases|null_new_deaths|\n",
            "+---------+------------+--------------+---------------+\n",
            "|        0|           0|          4348|          14532|\n",
            "+---------+------------+--------------+---------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aDy4JB7wh9Gz",
        "outputId": "664fd1f0-c21f-44c2-9565-0c42837bd104"
      },
      "source": [
        "# Select data, filling missing value and calculate cumulative of cases and deaths\n",
        "query = \"\"\"\n",
        "SELECT * \\\n",
        "  , SUM(new_cases) OVER(PARTITION BY country ORDER BY country, date) as cumulative_cases  \\\n",
        "  , SUM(new_deaths) OVER(PARTITION BY country ORDER BY country, date) as cumalative_deaths \\\n",
        "FROM\n",
        "  (SELECT date, YEAR(date) as year, MONTH(date) as month, location as country, IF( ISNULL(new_cases), 0, new_cases) as new_cases , IF( ISNULL(new_deaths), 0, new_deaths) as new_deaths FROM covid19)\n",
        "\"\"\"\n",
        "df_covid = spark.sql(query)\n",
        "df_covid.show()\n",
        "df_covid.count()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------+----+-----+-------+---------+----------+----------------+-----------------+\n",
            "|      date|year|month|country|new_cases|new_deaths|cumulative_cases|cumalative_deaths|\n",
            "+----------+----+-----+-------+---------+----------+----------------+-----------------+\n",
            "|2020-03-19|2020|    3|   Chad|      1.0|       0.0|             1.0|              0.0|\n",
            "|2020-03-20|2020|    3|   Chad|      0.0|       0.0|             1.0|              0.0|\n",
            "|2020-03-21|2020|    3|   Chad|      0.0|       0.0|             1.0|              0.0|\n",
            "|2020-03-22|2020|    3|   Chad|      0.0|       0.0|             1.0|              0.0|\n",
            "|2020-03-23|2020|    3|   Chad|      0.0|       0.0|             1.0|              0.0|\n",
            "|2020-03-24|2020|    3|   Chad|      2.0|       0.0|             3.0|              0.0|\n",
            "|2020-03-25|2020|    3|   Chad|      0.0|       0.0|             3.0|              0.0|\n",
            "|2020-03-26|2020|    3|   Chad|      0.0|       0.0|             3.0|              0.0|\n",
            "|2020-03-27|2020|    3|   Chad|      0.0|       0.0|             3.0|              0.0|\n",
            "|2020-03-28|2020|    3|   Chad|      0.0|       0.0|             3.0|              0.0|\n",
            "|2020-03-29|2020|    3|   Chad|      0.0|       0.0|             3.0|              0.0|\n",
            "|2020-03-30|2020|    3|   Chad|      2.0|       0.0|             5.0|              0.0|\n",
            "|2020-03-31|2020|    3|   Chad|      2.0|       0.0|             7.0|              0.0|\n",
            "|2020-04-01|2020|    4|   Chad|      0.0|       0.0|             7.0|              0.0|\n",
            "|2020-04-02|2020|    4|   Chad|      1.0|       0.0|             8.0|              0.0|\n",
            "|2020-04-03|2020|    4|   Chad|      0.0|       0.0|             8.0|              0.0|\n",
            "|2020-04-04|2020|    4|   Chad|      1.0|       0.0|             9.0|              0.0|\n",
            "|2020-04-05|2020|    4|   Chad|      0.0|       0.0|             9.0|              0.0|\n",
            "|2020-04-06|2020|    4|   Chad|      0.0|       0.0|             9.0|              0.0|\n",
            "|2020-04-07|2020|    4|   Chad|      1.0|       0.0|            10.0|              0.0|\n",
            "+----------+----+-----+-------+---------+----------+----------------+-----------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "106356"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ToEts0yoeWR"
      },
      "source": [
        "# Partitioning data by year"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CGsIOliOtw23",
        "outputId": "31dc280e-511f-486b-a9d2-a3368e4125fd"
      },
      "source": [
        "# Partitioning Dataframe with Year\n",
        "num_partition = df_covid.select('year').distinct().count()\n",
        "df_covid = df_covid.repartition( num_partition , 'year')\n",
        "\n",
        "print(df_covid.rdd.getNumPartitions())\n",
        "\n",
        "# Save output as CSV Files\n",
        "df_covid.write.csv('output', mode='overwrite', header=True)\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2SQMrfmIS18q",
        "outputId": "a7d30b11-1266-49b7-a004-0df2f6561241"
      },
      "source": [
        "# Test output\n",
        "df_output = spark.read.csv('output', header=True)\n",
        "\n",
        "df_output.show()\n",
        "df_output.count()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------+----+-----+-------+---------+----------+----------------+-----------------+\n",
            "|      date|year|month|country|new_cases|new_deaths|cumulative_cases|cumalative_deaths|\n",
            "+----------+----+-----+-------+---------+----------+----------------+-----------------+\n",
            "|2020-03-19|2020|    3|   Chad|      1.0|       0.0|             1.0|              0.0|\n",
            "|2020-03-20|2020|    3|   Chad|      0.0|       0.0|             1.0|              0.0|\n",
            "|2020-03-21|2020|    3|   Chad|      0.0|       0.0|             1.0|              0.0|\n",
            "|2020-03-22|2020|    3|   Chad|      0.0|       0.0|             1.0|              0.0|\n",
            "|2020-03-23|2020|    3|   Chad|      0.0|       0.0|             1.0|              0.0|\n",
            "|2020-03-24|2020|    3|   Chad|      2.0|       0.0|             3.0|              0.0|\n",
            "|2020-03-25|2020|    3|   Chad|      0.0|       0.0|             3.0|              0.0|\n",
            "|2020-03-26|2020|    3|   Chad|      0.0|       0.0|             3.0|              0.0|\n",
            "|2020-03-27|2020|    3|   Chad|      0.0|       0.0|             3.0|              0.0|\n",
            "|2020-03-28|2020|    3|   Chad|      0.0|       0.0|             3.0|              0.0|\n",
            "|2020-03-29|2020|    3|   Chad|      0.0|       0.0|             3.0|              0.0|\n",
            "|2020-03-30|2020|    3|   Chad|      2.0|       0.0|             5.0|              0.0|\n",
            "|2020-03-31|2020|    3|   Chad|      2.0|       0.0|             7.0|              0.0|\n",
            "|2020-04-01|2020|    4|   Chad|      0.0|       0.0|             7.0|              0.0|\n",
            "|2020-04-02|2020|    4|   Chad|      1.0|       0.0|             8.0|              0.0|\n",
            "|2020-04-03|2020|    4|   Chad|      0.0|       0.0|             8.0|              0.0|\n",
            "|2020-04-04|2020|    4|   Chad|      1.0|       0.0|             9.0|              0.0|\n",
            "|2020-04-05|2020|    4|   Chad|      0.0|       0.0|             9.0|              0.0|\n",
            "|2020-04-06|2020|    4|   Chad|      0.0|       0.0|             9.0|              0.0|\n",
            "|2020-04-07|2020|    4|   Chad|      1.0|       0.0|            10.0|              0.0|\n",
            "+----------+----+-----+-------+---------+----------+----------------+-----------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "106356"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    }
  ]
}